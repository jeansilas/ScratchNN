{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "_Up to this point, we´ve been only working with input and output layers. But in a normal Neural Network besides these layers, they have also __Hidden layers__, which are the ones where the magic mostly happens. We have been considering the layers for now as __Dense Layers__, which are the type of layers that fully connects to others.Further, we can see other types of layers_\n",
    "\n",
    "_In this notebook, the objective is to understand how to work with __Hidden Layers__, in special the __Dense Layers__ and how the process can be automated by turning it all in a class._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/hiddden_layer.jpg\" width=400 height=400/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre> </pre>\n",
    "_We already know how to do the calculations to obtain the output of a neuron and how to use Numpy to simplify the process. However, so far, we need to initialize the values of __Weights__ and __Biases__, besides turning them into proper arrays for our purpose. So now, in order to work better with these layers, let´s make use of OOP ( Object Oriented Programming ) to automate all of this previous work. But before we jump off to __Classes__, let´s take a look at some Numpy functions that will be useful for us._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $np.random.randn()$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This function generates random values that follow a Gaussian Distribution of Mean 0 and Variance 1. Its input is the shape of the array you want to receive and the output is the array with the shape of your choice with random values that \"obey\" the Gaussian Distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>import numpy as np</code>\n",
    "\n",
    "<code>Ex1 = np.random.randn(2,5)</code>\n",
    "\n",
    "<code>Ex1 = [[ 1.7640524,0.4001572, 0.978738, 2.2408931, 1.867558 ],\n",
    "[ - 0.9772779, 0.95008844, - 0.1513572, - 0.10321885, 0.41059852 ]]\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $np.zeros()$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This function fill all values of an array - with a shape of your choice - with zeros. This way, the input of this function is a tuple of the shape you want for the array and output is this array filled with zero values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code> import numpy as np </code>\n",
    "\n",
    "<code> Ex1 = np.zeros((2,5))</code>\n",
    "\n",
    "<code> Ex1 = [[0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0]] </code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we went through these numpy functions, we can __finally__ go the classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer Class\n",
    "\n",
    "<pre></pre>\n",
    "* Let´s firt define a class, then we bulid its construtor function and finally the foward function, which is the one responsible for the calculations of that layer and its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "\n",
    "#Creating the class\n",
    "class Dense_Layer():\n",
    "    \n",
    "    #Function responsible for intiliaze all of the important attributes of the class\n",
    "    # n_input -> number of features that input layer has\n",
    "    # n_neurons -> number os neurons in this layer\n",
    "    def __init__(self,n_input,n_neurons):\n",
    "        pass\n",
    "    \n",
    "    #function responsible for the calculations and the layer output\n",
    "    def forward(self,inputs):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The code above is just a empty structure, it is not functional yet. We need to fill, so that it can actually automate the process we have been doing so far._\n",
    "\n",
    "So the next steps are:\n",
    "\n",
    "   * Initialize the random values of weight in a proper array\n",
    "    \n",
    "   * Initialize the values of the biases as zero \n",
    "    \n",
    "   * Fill the __forward__ function so that it performs what it was designed to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "\n",
    "#Creating the class\n",
    "class Dense_Layer():\n",
    "    \n",
    "    #Function responsible for intiliaze all of the important attributes of the class\n",
    "    # n_input -> number of features that input layer has\n",
    "    # n_neurons -> number os neurons in this layer\n",
    "    def __init__(self,n_input,n_neurons):\n",
    "        # The biases is an array of 1D-dimension which the number size the same as the quantity of neurons in the layer\n",
    "        self.biases = np.ones((1,n_neurons))\n",
    "        \n",
    "        #The weights is an array of 2D-dimension which the shape follows this rule: (n_neurons,number of neurons of the precious layer)\n",
    "        # It gets mutlipleid by 0.01 so that the weights initially don´t have a great impact in the NN\n",
    "        self.weights = 0.01*np.random.randn(n_input,n_neurons)\n",
    "        \n",
    "    #function responsible for the calculations and the layer output\n",
    "    def forward(self,inputs):\n",
    "        outputs = np.dot(inputs,self.weights.T) + self.biases\n",
    "        \n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Well, our class now seem to be funtional, therefore let´s test it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.12412231 1.0036332  0.81385942 1.01253295]\n",
      " [1.54597912 1.26653052 0.92483117 2.2823601 ]\n",
      " [1.04288315 0.91061299 0.95001689 0.94869812]]\n"
     ]
    }
   ],
   "source": [
    "# Batch of inputs\n",
    "inputs = [[27.0,3.0,12.0,4.0],[16.0,97.0,82.0,7.0],[21.0,-1.2,0.8,0.0]]\n",
    "\n",
    "#n_input -> number of features of the input\n",
    "n_input = 4\n",
    "\n",
    "#n_neurons -> number of neurons for that layer\n",
    "n_neurons = 4\n",
    "\n",
    "# Initialize the Dense Layer\n",
    "layer_1 = Dense_Layer(n_input,n_neurons)\n",
    "\n",
    "# Perform the forward pass ( calculations that give us the output array )\n",
    "outputs = layer_1.forward(inputs)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __It´s working! wonderful, isn´t it ? But what about making just a little change to let the code more coherent.__\n",
    "\n",
    "> __If you take a look at the weight array, instead of initialing it in the way it is being initalized, we could actually begin with its transposition, since we'll only use it for now this way.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "\n",
    "#Creating the class\n",
    "class Dense_Layer():\n",
    "    \n",
    "    #Function responsible for intiliaze all of the important attributes of the class\n",
    "    # n_input -> number of features that input layer has\n",
    "    # n_neurons -> number os neurons in this layer\n",
    "    def __init__(self,n_input,n_neurons):\n",
    "        # The biases is an array of 1D-dimension which the number size the same as the quantity of neurons in the layer\n",
    "        self.biases = np.ones((1,n_neurons))\n",
    "        \n",
    "        #The weights is an array of 2D-dimension which the shape follows this rule: (n_neurons,number of neurons of the precious layer), but we switched the values because we need its tranposition.\n",
    "        # It gets mutlipleid by 0.01 so that the weights initially don´t have a great impact in the NN\n",
    "        self.weights = 0.01*np.random.randn(n_neurons,n_input)\n",
    "        \n",
    "    #function responsible for the calculations and the layer output\n",
    "    def forward(self,inputs):\n",
    "        outputs = np.dot(inputs,self.weights) + self.biases\n",
    "        \n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.77001681 0.89193493 0.67800826 0.72321197]\n",
      " [1.88805683 0.48143198 1.16322966 1.0310517 ]\n",
      " [0.7617869  0.99198013 0.77771409 0.82520383]]\n"
     ]
    }
   ],
   "source": [
    "# Batch of inputs\n",
    "inputs = [[27.0,3.0,12.0,4.0],[16.0,97.0,82.0,7.0],[21.0,-1.2,0.8,0.0]]\n",
    "\n",
    "#n_input -> number of features of the input\n",
    "n_input = 4\n",
    "\n",
    "#n_neurons -> number of neurons for that layer\n",
    "n_neurons = 4\n",
    "\n",
    "# Initialize the Dense Layer\n",
    "layer_1 = Dense_Layer(n_input,n_neurons)\n",
    "\n",
    "# Perform the forward pass ( calculation that gives us the output array )\n",
    "outputs = layer_1.forward(inputs)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### Now let´s move on to adding more layers..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *_So far, we have only worked with the input layer, but now let´s make use of an input function and two hidden layers, in this case Dense Layers._*\n",
    " \n",
    " <pre> \n",
    " \n",
    " </pre>\n",
    " \n",
    " <img src=\"Images/hidden_layer2.png\" width=400 height=400 />\n",
    " \n",
    " <pre>\n",
    " \n",
    " </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In order to follow the flow in the right way, we must recall to these rules below:**\n",
    "    \n",
    "   * The number of weights of a neuron must be equal to the number of neurons of the previous layer, if the previous layer is the input layer then number of of weight of a neuron will be the number of features of the input.\n",
    "   <pre></pre>\n",
    "   \n",
    "   * The number of biases of a layer in the number of neurons of that layer\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.02102372 1.00302874 0.9770972  1.01831999]\n",
      " [1.01771215 1.02132182 0.99140993 1.01382075]\n",
      " [1.01918632 1.0026599  0.97920781 1.02057674]]\n"
     ]
    }
   ],
   "source": [
    "# Batch of inputs\n",
    "inputs = [[27.0,3.0,12.0,4.0],[16.0,97.0,82.0,7.0],[21.0,-1.2,0.8,0.0]]\n",
    "\n",
    "#n_input -> number of features of the input\n",
    "n_input = 4\n",
    "\n",
    "#n_neurons -> number of neurons for that layer\n",
    "n_neurons = 4\n",
    "\n",
    "# Initialize the Dense Layer\n",
    "layer_1 = Dense_Layer(n_input,n_neurons)\n",
    "\n",
    "# Perform the forward pass ( calculation that gives us the output array )\n",
    "output_1 = layer_1.forward(inputs)\n",
    "\n",
    "# Initialize The other Dense Layer\n",
    "\n",
    "#Number of neurons for the second hidden layer\n",
    "n_neurons_2 = 4\n",
    "# Number of features for the input: .shape, outputs a tuple with the shape of the array.\n",
    "n_input_2 = output_1.shape[1]\n",
    "\n",
    "layer_2 = Dense_Layer(n_input_2,n_neurons_2)\n",
    "\n",
    "output_2 = layer_2.forward(output_1)\n",
    "\n",
    "print(output_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _To sum up, as it can be seen, we have almost the structure of a basic Neural Network functioning. But it still needs the **activation functions** to have the complete basic structure._ \n",
    "<pre></pre>\n",
    "<br>In the next notebook, we will learn about the **activation functions**.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
