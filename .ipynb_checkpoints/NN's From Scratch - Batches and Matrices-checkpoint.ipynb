{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _In the Neural Networks world, inputs come in the form of batches, which is different of what´s been done so far._ \n",
    " \n",
    " <br>_But why that ?_</br>\n",
    " <br></br>\n",
    " \n",
    " Because of two main reasons:\n",
    " \n",
    " * The model gets trained faster in this parallel process\n",
    " \n",
    " * It helps with the __generalization__ of the model during the training, what avoids __overfitting__\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point the input was coming under this form:\n",
    "\n",
    "<code>input = [1,3,5,7]</code>\n",
    "\n",
    "The use of this form only trains the model one sample by sample.\n",
    "\n",
    "But when the inputs come in __batches__, it looks this way:\n",
    "\n",
    "<code>inputs = [[1,3,5,7],\n",
    "          [2,6,10,14],\n",
    "          [3,9,15,21],\n",
    "          [4,12,20,28]]</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the reason behind the use of batches was comprehended, now it´s time to take a look at what matrices can do for us. If you paid attention to the form of the array, you know that the inputs are no longer _1D-arrays_; now they´re _2D-arrays_ ,or most known as __matrices__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices ( once again )\n",
    "\n",
    "<pre>\n",
    "\n",
    "</pre>\n",
    "\n",
    "\n",
    "* In the previous notebook, in order to do the _\"NN´s math\"_, we were using the properties of arrays of performing **dot products**, however, now, besides weights, inputs come as matrices as well. So now, we´re not dealing no more with vectors, this way, in order to do the **dot product** between these two _2D-arrays_, we'll cover a bit of  __Matrix product and Transpose of a Matrix__ \n",
    "\n",
    " <br>    *looks like Algebra Linear makes sense now, doesn´t it ? ;)*     </br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Product \n",
    "\n",
    "> _Matrix Product is an operation in which we have 2 matrices, and we are performing **dot products** of all combinations of rows from the first matrix and the columns of the 2nd matrix, resulting in a matrix of those atomic **dot products**_\n",
    "\n",
    "_Definition taken from the book: Neural Networks From Scratch in Python  by Harryson Kinsley, Daniel Kukiela._\n",
    "\n",
    "For this operation to occur, The matrices must follow a rule:\n",
    "\n",
    "* 1st Matrice: $A$ $->$ Shape ($m$ x $n$)\n",
    "* 2nd Matrice: $B$ $->$ Shape ($p$ x $q$)\n",
    "\n",
    "if you want to calculate $A x B$, then:\n",
    "\n",
    "* $n$ must be equal to $p$\n",
    "\n",
    "__This operation would result in a matrice of ($m$ x $q$) shape__\n",
    "\n",
    "Or if you want to calculate $B x A$, then:\n",
    "\n",
    "* $m$ must be equal to $q$\n",
    "\n",
    "__This operation would result in a matrice of ($p$ x $n$) shape__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <code>\n",
    "    A = [[1,2,3],[1,3,5]]\n",
    "    B = [[2,1],[5,2],[1,6]]\n",
    "</code>\n",
    "\n",
    "* $AxB$:\n",
    "<code>\n",
    "    A_x_B = [[15,23],[22,37]]\n",
    "</code>\n",
    "\n",
    "* $BxA$:\n",
    "<code>\n",
    "    B_x_A = [[3,7,11],[7,16,25],[7,20,33]]\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this Matrix´s rule, not always will be possible to perfom a Matrix Product between a Input batch and Weights matrix, without making some modifications in one of the Matrices. This modification is usually done in The Weight matrix and that modification is the __Matrix´s Transpose__, what allows us to keep on performing the Matrix Prouct in these ocasions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposition fo a Matrix\n",
    "\n",
    "> _It is a operation in which the arrows and columns of a matrix switch with each other._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exemplo:\n",
    "\n",
    "$A$ $->$ Shape: ($3$ x $2$)\n",
    "\n",
    "<code>\n",
    "    A = [[2,1],[5,2],[1,6]]\n",
    "    A_T = np.array(A).T\n",
    "    A_T = [[1,2,6],[2,5,1]]\n",
    "</code>\n",
    "\n",
    "$A^T$ $->$ Shape: ($2$ x $3$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Now that the concepts of this notebook jupyter were all covered, let´s show how it´d work to do NN´s calculations making use of these new tools we´ve acquired._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers of Neurons, Numpy and Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.9     7.39    7.58 ]\n",
      " [-11.9    -4.9    59.83 ]\n",
      " [  4.52    3.576   2.294]]\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "\n",
    "# Input Values of a single neuron\n",
    "inputs = [[27.0,3.0,12.0,4.0],[16.0,97.0,82.0,7.0],[21.0,-1.2,0.8,0.0]]\n",
    "\n",
    "# Random values for Weights\n",
    "weight_1 = [0.1,-0.5,0.4,0.1]\n",
    "weight_2 = [0.14,-0.17,0.04,0.81]\n",
    "weight_3 = [0.21,0.33,0.35,-0.32]\n",
    "\n",
    "# Random Values for Bias\n",
    "bias_1 = 1.5\n",
    "bias_2 = 0.4\n",
    "bias_3 = -2.0\n",
    "\n",
    "# Let´s put the weights and biases in lists:\n",
    "weights = [weight_1,weight_2,weight_3]\n",
    "biases = [bias_1,bias_2,bias_3]\n",
    "\n",
    "# Let´s calculate the outputs\n",
    "# But before the performation of the matrix product, we must transpose the weight matrix\n",
    "weights_T = np.array(weights).T\n",
    "\n",
    "outputs = np.dot(inputs, weights_T) + biases\n",
    "\n",
    "# Displaying the output result\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As it can be seen, we have a group of outputs, which is great to accelerate the training model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
