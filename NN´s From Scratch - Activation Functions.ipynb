{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Activactions functions are responsible for mapping the predicition that Neural Networks make according to the input they receive. The functioning of an __activation function__ is based on **\"turning on\"** or **\"turning off\"**( or how probable it is to turn on or turn off depending on the type of activation function) a speficic neuron on the entiral Network, determined by its calculation below:\n",
    " <pre>\n",
    " \n",
    " </pre>\n",
    "$output = Activation \\ Function (input \\cdot weight+ bias)$\n",
    "\n",
    "<pre>\n",
    "\n",
    "</pre>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here´s a Gif to demonstrate how it actually works:\n",
    "\n",
    "<pre></pre>\n",
    "\n",
    "<img src=\"Images/NN_Gif.gif\" width= 400 height=400 />\n",
    "\n",
    "<pre></pre>\n",
    "* The neurons in green are those that were activated for that specific input\n",
    "* The neurons that aren´t colored in green are deactivated for that specific input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In the image below, there´s an illustration of what´s the structure of NN with activation function like:__\n",
    "\n",
    "<pre>\n",
    "\n",
    "</pre>\n",
    "\n",
    "<img src=\"Images/Activation_Function_model.JPEG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Let´s see some types of activations functions_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Step Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function was one of the first activation functions that were used for Neural Networks, but nowadays they are not used anymore because it is not useful in terms of NN´s learning. That´s due to its failure to catch how close an input was to activate a neuron, if the value was $-1$ or $-300$, it will always have the same output without actually expliciting to the model the closeness of turning a neuro on.\n",
    "\n",
    "_Here´s how a picture of how it works:_\n",
    "\n",
    "<img src=\"Images/Step_function.PNG\" width = 600 height = 600 />\n",
    "\n",
    "*  ___Summaring, if a neuron´s output is less or equal to zero, it´ll output $0$, otherwise it´ll output $1$___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This activation function is mostly used in the output layer of regression model of neural networks. it´s simply a $y=x$ function\n",
    "\n",
    "_Here´s a picture of how it works:_\n",
    "<pre>\n",
    "\n",
    "</pre>\n",
    "\n",
    "<img src=\"Images/Linear_Function.PNG\" width=600 height=600/>\n",
    "<pre>\n",
    "\n",
    "</pre>\n",
    "\n",
    "* ___Summaring, it´ll output the neurons result (output)___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function that tends to $0$ when its input goes to negative infinity, that reaches $0.5$ when its inputs is $0$ and tends to $1$ when its input goes to positive infinity. This function is really good for NN´s learning, since it overcomes the failure of not analizing how close a neuron was to activate or not. There´s just one small disadvantage for this function, it doesn´t deal well with __dead neurons__ which are the types of neuron that keeps outputting zero due to its weight. This Activation function also brings the functionality of dealing with non-linear data. But after some time, it was replaced for the **ReLu**  activation function, because **ReLU** is easier to implement computationally and has the same advantages.\n",
    "\n",
    "_Here´s an example of how it works:_\n",
    "<pre>\n",
    "\n",
    "</pre>\n",
    "\n",
    "\n",
    "<img src=\"Images/Sigmoid_Function.PNG\" width=600 height=600/>\n",
    "<pre>\n",
    "\n",
    "</pre>\n",
    "\n",
    "* ___Summaring, it ouputs values between $\\ 0 $ and $\\ 1$, moreover when neuron´s output is $0$ its result is $0.5$___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rectified Linear  Unit Activation Function (ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This activation Function is one the the mostly used in the actual days, since it can deal with non-linear data, supports the NN´s learning process and it´s easier to implement than **Sigmoid Activation Function**. it actually is very similiar to the **Step Activation Function**, just instead of outputting $\\ 1$ when the neuron´s input is greater than $\\ 0$, it outputs the input itself.\n",
    "\n",
    "_Here´s an example of how it works:_\n",
    "<pre>\n",
    "\n",
    "</pre>\n",
    "\n",
    "\n",
    "<img src=\"Images/ReLu_function.PNG\" width=600 height=600/>\n",
    "<pre>\n",
    "\n",
    "</pre>\n",
    "\n",
    "* ___Summaring, when the neuron´s output is less or equal to $\\ 0$, it deactivates, otherwise it outputs the neuron´s output___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Softmax Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This activaction function is really important for classification models, since it can receive non-normalized data and outputs a distribution of probabilities which is normalized. This one has a more complicated math associated with it, but with programming there´s no big deal, moreover with the use of Numpy library its calculation can get faster and more optimized.\n",
    "\n",
    "_Here´s an example of how it works:_\n",
    "\n",
    "<pre>\n",
    "\n",
    "</pre>\n",
    "\n",
    "<img src=\"Images/softmax.PNG\" />\n",
    "\n",
    "<pre>\n",
    "\n",
    "</pre>\n",
    "\n",
    "it is a bit confusing, isn´t it ?\n",
    "\n",
    "Well, by breaking down this function into small chunks, it can be provided a better understanding of what this function does, taking all the confusion away.\n",
    "\n",
    "\n",
    "In the general, this function gets each result of a neuron in layer, and put it as expoent of a exponenttial of base __e - Euler number__. Then it sums all of these exponentiated values, and divide each of these exponential values by this sum. \n",
    "\n",
    "__It´ll get even clearer in the code section.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now, let´s go to the fun part : **Code Time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, let´s take a more pragmatical approach by coding the Activation Functions\n",
    "\n",
    "# But first, let´s settle down the background to to make use of the activation functions.\n",
    "\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "\n",
    "#Creating the class\n",
    "class Dense_Layer():\n",
    "    \n",
    "    #Function responsible for intiliaze all of the important attributes of the class\n",
    "    # n_input -> number of features that input layer has\n",
    "    # n_neurons -> number os neurons in this layer\n",
    "    def __init__(self,n_input,n_neurons):\n",
    "        # The biases is an array of 1D-dimension which the number size the same as the quantity of neurons in the layer\n",
    "        self.biases = np.ones((1,n_neurons))\n",
    "        \n",
    "        #The weights is an array of 2D-dimension which the shape follows this rule: (n_neurons,number of neurons of the precious layer), but we switched the values because we need its tranposition.\n",
    "        # It gets mutlipleid by 0.01 so that the weights initially don´t have a great impact in the NN\n",
    "        self.weights = 0.01*np.random.randn(n_neurons,n_input)\n",
    "        \n",
    "    #function responsible for the calculations and the layer output\n",
    "    def forward(self,inputs):\n",
    "        outputs = np.dot(inputs,self.weights) + self.biases\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "\n",
    "# Batch of inputs\n",
    "inputs = [[27.0,3.0,12.0,4.0],[16.0,97.0,82.0,7.0],[21.0,-1.2,0.8,0.0]]\n",
    "\n",
    "#n_input -> number of features of the input\n",
    "n_input = 4\n",
    "\n",
    "#n_neurons -> number of neurons for that layer\n",
    "n_neurons = 4\n",
    "\n",
    "# Initialize the Dense Layer\n",
    "layer_1 = Dense_Layer(n_input,n_neurons)\n",
    "\n",
    "# Perform the forward pass ( calculation that gives us the output array )\n",
    "output_1 = layer_1.forward(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Now, it´s activation time!! Let´s jump right into them.\n",
    "\n",
    "# The Step Activaction Function\n",
    "\n",
    "class Step_Activation():\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        self.output = np.zeros((np.array(inputs).shape[0],np.array(inputs).shape[1]))\n",
    "        \n",
    "        for i,input_ in enumerate(inputs):\n",
    "            for e,input__ in enumerate(input_):\n",
    "                if input__ > 0:\n",
    "                    self.output[i][e] = 1\n",
    "        return self.output\n",
    "        \n",
    "\n",
    "Activation_1 = Step_Activation()\n",
    "output = Activation_1.forward([[-1,-1,-1],[2,2,2],[3,-1,0]])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1 -1 -1]\n",
      " [ 2  2  2]\n",
      " [ 3 -1  0]]\n"
     ]
    }
   ],
   "source": [
    "# The Linear Activation Function \n",
    "\n",
    "class Linear_Activation():\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        \n",
    "        return np.array(inputs)\n",
    "    \n",
    "Activation_1 = Linear_Activation()\n",
    "output = Activation_1.forward([[-1,-1,-1],[2,2,2],[3,-1,0]])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26894142 0.26894142 0.26894142]\n",
      " [0.88079708 0.88079708 0.88079708]\n",
      " [0.95257413 0.26894142 0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "# The Sigmoid Activation Function\n",
    "\n",
    "class Sigmoid_Activation():\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        self.output = np.zeros((np.array(inputs).shape[0],np.array(inputs).shape[1]))\n",
    "        inputs = np.array(inputs)\n",
    "        \n",
    "        self.output = 1/(1+np.exp(-inputs))\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "Activation_1 = Sigmoid_Activation()\n",
    "output = Activation_1.forward([[-1,-1,-1],[2,2,2],[3,-1,0]])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [2. 2. 2.]\n",
      " [3. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# The ReLU Activation Function\n",
    "\n",
    "class ReLU_Activation():\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        self.output = np.zeros((np.array(inputs).shape[0],np.array(inputs).shape[1]))\n",
    "        inputs = np.array(inputs)\n",
    "        \n",
    "        for i,input_ in enumerate(inputs):\n",
    "            for e,input__ in enumerate(input_):\n",
    "                if input__ > 0:\n",
    "                    self.output[i][e] = input__\n",
    "        \n",
    "        \n",
    "        return self.output\n",
    "\n",
    "Activation_1 = ReLU_Activation()\n",
    "output = Activation_1.forward([[-1,-1,-1],[2,2,2],[3,-1,0]])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [2 2 2]\n",
      " [3 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# There´s a better way to do this in Numpy:\n",
    "\n",
    "# The ReLU Activation Function \n",
    "\n",
    "class ReLU_Activation():\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        inputs = np.array(inputs)\n",
    "        \n",
    "        self.output = np.maximum(0,inputs)\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "Activation_1 = ReLU_Activation()\n",
    "output = Activation_1.forward([[-1,-1,-1],[2,2,2],[3,-1,0]])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.93623955 0.01714783 0.04661262]]\n"
     ]
    }
   ],
   "source": [
    "# The Softmax Activation Function\n",
    "\n",
    "class Softmax_Activation():\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        inputs = np.array(inputs)\n",
    "        \n",
    "        exp_list = np.exp(inputs)\n",
    "        \n",
    "        probabilities = exp_list/np.sum(exp_list, axis=1, keepdims=True)\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "Activation_1 = Softmax_Activation()\n",
    "output = Activation_1.forward([[-1,-1,-1],[2,2,2],[3,-1,0]])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There´s just one small problem we need to solve. The Softmax Activation takes as its input non-normalized values, so sometimes it can get high values. Regarding the fact that Softmax works with exponential function, it can be problematic due to overflow of values. Let me show it in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22026.465794806718\n"
     ]
    }
   ],
   "source": [
    "value_1 = 10\n",
    "value_2 = 100\n",
    "value_3 = 1000\n",
    "value_4 = 10000\n",
    "\n",
    "print(np.exp(value_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6881171418161356e+43\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(value_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-7505c43c355a>:1: RuntimeWarning: overflow encountered in exp\n",
      "  print(np.exp(value_3))\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(value_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-65-af0af11c1aa1>:1: RuntimeWarning: overflow encountered in exp\n",
      "  print(np.exp(value_4))\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(value_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__So as it can be seen, we need to find a way to overcome this issue.__\n",
    "\n",
    "We can do that, by subtracting the highest value in the array, since we´re dealing with exponenial function and we´re normaling the valuea at the end.\n",
    "\n",
    "So let´s do that in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.33333333  0.33333333  0.33333333]\n",
      " [ 0.33333333  0.33333333  0.33333333]\n",
      " [ 1.37195581 -0.21135731 -0.1605985 ]]\n"
     ]
    }
   ],
   "source": [
    "# The Softmax Activation Function\n",
    "\n",
    "class Softmax_Activation():\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        inputs = np.array(inputs)\n",
    "        \n",
    "        exp_list = (np.exp(inputs)-np.max(inputs,axis=1,keepdims=True))\n",
    "        \n",
    "        probabilities = exp_list/np.sum(exp_list, axis=1, keepdims=True)\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "Activation_1 = Softmax_Activation()\n",
    "output = Activation_1.forward([[-1,-1,-1],[2,2,2],[3,-1,0]])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to end this notebook, let´s re-do the proccess we did previouly but making use of the __Activation Functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "\n",
    "#Creating the class\n",
    "class Dense_Layer():\n",
    "    \n",
    "    #Function responsible for intiliaze all of the important attributes of the class\n",
    "    # n_input -> number of features that input layer has\n",
    "    # n_neurons -> number os neurons in this layer\n",
    "    def __init__(self,n_input,n_neurons):\n",
    "        # The biases is an array of 1D-dimension which the number size the same as the quantity of neurons in the layer\n",
    "        self.biases = np.ones((1,n_neurons))\n",
    "        \n",
    "        #The weights is an array of 2D-dimension which the shape follows this rule: (n_neurons,number of neurons of the precious layer), but we switched the values because we need its tranposition.\n",
    "        # It gets mutlipleid by 0.01 so that the weights initially don´t have a great impact in the NN\n",
    "        self.weights = 0.01*np.random.randn(n_neurons,n_input)\n",
    "        \n",
    "    #function responsible for the calculations and the layer output\n",
    "    def forward(self,inputs):\n",
    "        outputs = np.dot(inputs,self.weights) + self.biases\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# The ReLU Activation Function \n",
    "\n",
    "class ReLU_Activation():\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        inputs = np.array(inputs)\n",
    "        \n",
    "        self.output = np.maximum(0,inputs)\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "\n",
    "# The Softmax Activation Function\n",
    "\n",
    "class Softmax_Activation():\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        inputs = np.array(inputs)\n",
    "        \n",
    "        exp_list = (np.exp(inputs)-np.max(inputs,axis=1,keepdims=True))\n",
    "        \n",
    "        probabilities = exp_list/np.sum(exp_list,axis=1, keepdims=True)\n",
    "        \n",
    "        return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23221253 0.2647992  0.24480698 0.25818129]\n",
      " [0.23652323 0.2677135  0.23793666 0.25782661]\n",
      " [0.23268647 0.26349917 0.24750048 0.25631387]]\n"
     ]
    }
   ],
   "source": [
    "# Batch of inputs\n",
    "inputs = [[27.0,3.0,12.0,4.0],[16.0,97.0,82.0,7.0],[21.0,-1.2,0.8,0.0]]\n",
    "\n",
    "#n_input -> number of features of the input\n",
    "n_input = 4\n",
    "\n",
    "#n_neurons -> number of neurons for that layer\n",
    "n_neurons = 4\n",
    "\n",
    "# Initialize the Dense Layer\n",
    "layer_1 = Dense_Layer(n_input,n_neurons)\n",
    "\n",
    "# Perform the forward pass ( calculation that gives us the output array )\n",
    "output_1 = layer_1.forward(inputs)\n",
    "\n",
    "# initialize the ReLU activation\n",
    "activation_1 = ReLU_Activation() \n",
    "\n",
    "# Perform the the activation through the Forward pass\n",
    "\n",
    "output_1_activacted = activation_1.forward(output_1)\n",
    "\n",
    "# Initialize The other Dense Layer\n",
    "\n",
    "#Number of neurons for the second hidden layer\n",
    "n_neurons_2 = 4\n",
    "# Number of features for the input: .shape, outputs a tuple with the shape of the array.\n",
    "n_input_2 = output_1_activacted.shape[1]\n",
    "\n",
    "layer_2 = Dense_Layer(n_input_2,n_neurons_2)\n",
    "\n",
    "output_2 = layer_2.forward(output_1_activacted)\n",
    "\n",
    "# Initialize the Softmax Activation\n",
    "\n",
    "activation_2 = Softmax_Activation()\n",
    "\n",
    "# Perform the the activation through the Forward pass\n",
    "\n",
    "output_2_activacted = activation_2.forward(output_2)\n",
    "\n",
    "print(output_2_activacted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have approched the __Activation Functions__, we have basic structure of a Neural Network.\n",
    "\n",
    "The next step is to see how our model is going, for that we need to calculate the __loss__ of it. So that we have the right tools to adjust our model by analizing this __loss__ and changing the __weights and bias__ values. \n",
    "\n",
    "Perhaps, you´re starting to see the big picture of NN´s. If not, don´t worry, we still have a lot to cover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the next notebook, we are going to learn about the __Loss__ in a Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
